{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9285625b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "from scipy.io import loadmat\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg\n",
    "import cv2\n",
    "from collections import deque\n",
    "from pinkrigs_tools.dataset.query import load_data, queryCSV\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "import glob\n",
    "from utils.pipeline import load_and_process_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "088eece0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "subject_id = 'AV043'\n",
    "date = '2024-03-05'\n",
    "exp_kwargs = {\n",
    "    'subject': subject_id,\n",
    "    'expDate': date,\n",
    "    }\n",
    "\n",
    "\n",
    "input_path = fr\"\\\\znas\\Lab\\Share\\Maja\\labelled_DLC_videos\\{subject_id}_{date}.mp4\"\n",
    "output_path = f\"H:/Annotated_Videos/annotated_video_{subject_id}_{date}.mp4\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84583456",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_plots(oa_speed, wh_speed, dpi=80):\n",
    "    \n",
    "    # Create figures\n",
    "    fig_oa, ax_oa = plt.subplots(figsize=(8, 3), dpi=dpi)\n",
    "    fig_wh, ax_wh = plt.subplots(figsize=(8, 3), dpi=dpi)\n",
    "    \n",
    "    # Plot the main lines\n",
    "    x_all = np.arange(len(oa_speed))\n",
    "    line_oa, = ax_oa.plot(x_all, oa_speed, 'b-')\n",
    "    line_wh, = ax_wh.plot(x_all, wh_speed, 'g-')\n",
    "    \n",
    "    # Set y limits\n",
    "    y_lim_oa = np.nanmax(oa_speed) if len(oa_speed) > 0 else 1\n",
    "    y_lim_wh = np.nanmax(wh_speed) if len(wh_speed) > 0 else 1\n",
    "    ax_oa.set_ylim(0, y_lim_oa * 1.1)\n",
    "    ax_wh.set_ylim(0, y_lim_wh * 1.1)\n",
    "    \n",
    "    # Create canvases for faster rendering\n",
    "    canvas_oa = FigureCanvasAgg(fig_oa)\n",
    "    canvas_wh = FigureCanvasAgg(fig_wh)\n",
    "    \n",
    "    # Return everything we need\n",
    "    return (fig_oa, ax_oa, canvas_oa, line_oa, y_lim_oa,\n",
    "            fig_wh, ax_wh, canvas_wh, line_wh, y_lim_wh)\n",
    "\n",
    "def update_plot_window(ax_oa, ax_wh, line_oa, line_wh, y_lim_oa, y_lim_wh,\n",
    "                      window_start, window_end, oa_onsets, oa_offsets, \n",
    "                      wh_onsets, wh_offsets):\n",
    "    \n",
    "    # Clear previous annotations but keep the main lines\n",
    "    ax_oa.clear()  # Clear everything\n",
    "    ax_oa.plot(line_oa.get_xdata(), line_oa.get_ydata(), 'b-')  # Redraw the line\n",
    "    ax_oa.set_ylim(0, y_lim_oa * 1.1)\n",
    "    \n",
    "    ax_wh.clear()  # Clear everything\n",
    "    ax_wh.plot(line_oa.get_xdata(), line_oa.get_ydata(), 'b-')  # Redraw the line\n",
    "    ax_wh.set_ylim(0, y_lim_oa * 1.1)\n",
    "    \n",
    "    # Update x limits\n",
    "    ax_oa.set_xlim(window_start, window_end)\n",
    "    ax_wh.set_xlim(window_start, window_end)\n",
    "    \n",
    "    # Filter indices for current window\n",
    "    oa_onset_indices = oa_onsets[(oa_onsets >= window_start) & (oa_onsets <= window_end)]\n",
    "    oa_offset_indices = oa_offsets[(oa_offsets >= window_start) & (oa_offsets <= window_end)]\n",
    "    wh_onset_indices = wh_onsets[(wh_onsets >= window_start) & (wh_onsets <= window_end)]\n",
    "    wh_offset_indices = wh_offsets[(wh_offsets >= window_start) & (wh_offsets <= window_end)]\n",
    "    \n",
    "    # Add annotations for OA\n",
    "    for onset_idx in oa_onset_indices:\n",
    "        ax_oa.axvline(onset_idx, color='black', linestyle='--', alpha=0.7)\n",
    "        oa_distances = oa_offset_indices - onset_idx\n",
    "        valid_offsets = np.where(oa_distances > 0)[0]\n",
    "        \n",
    "        if len(valid_offsets) > 0:\n",
    "            next_offset_idx = oa_offset_indices[valid_offsets[0]]\n",
    "            x = np.arange(onset_idx, next_offset_idx + 1)\n",
    "            ax_oa.fill_between(x, 0, y_lim_oa, alpha=0.3, color='green')\n",
    "    \n",
    "    for offset_idx in oa_offset_indices:\n",
    "        ax_oa.axvline(offset_idx, color='black', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add annotations for WH\n",
    "    for onset_idx in wh_onset_indices:\n",
    "        ax_wh.axvline(onset_idx, color='black', linestyle='--', alpha=0.7)\n",
    "        wh_distances = wh_offset_indices - onset_idx\n",
    "        valid_offsets = np.where(wh_distances > 0)[0]\n",
    "        \n",
    "        if len(valid_offsets) > 0:\n",
    "            next_offset_idx = wh_offset_indices[valid_offsets[0]]\n",
    "            x = np.arange(onset_idx, next_offset_idx + 1)\n",
    "            ax_wh.fill_between(x, 0, y_lim_wh, alpha=0.3, color='purple')\n",
    "    \n",
    "    for offset_idx in wh_offset_indices:\n",
    "        ax_wh.axvline(offset_idx, color='black', linestyle='--', alpha=0.7)\n",
    "\n",
    "def get_plot_images(canvas_oa, canvas_wh):\n",
    "    \"\"\"REPLACES fig_to_image: Uses canvas for faster conversion\"\"\"\n",
    "    \n",
    "    canvas_oa.draw()\n",
    "    canvas_wh.draw()\n",
    "    \n",
    "    buf_oa = np.frombuffer(canvas_oa.tostring_rgb(), dtype=np.uint8)\n",
    "    buf_wh = np.frombuffer(canvas_wh.tostring_rgb(), dtype=np.uint8)\n",
    "    \n",
    "    w_oa, h_oa = canvas_oa.get_width_height()\n",
    "    w_wh, h_wh = canvas_wh.get_width_height()\n",
    "    \n",
    "    img_oa = buf_oa.reshape(h_oa, w_oa, 3)\n",
    "    img_wh = buf_wh.reshape(h_wh, w_wh, 3)\n",
    "    \n",
    "    img_oa_bgr = cv2.cvtColor(img_oa, cv2.COLOR_RGB2BGR)\n",
    "    img_wh_bgr = cv2.cvtColor(img_wh, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    return img_oa_bgr, img_wh_bgr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3cc4bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_video_frames(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    if not cap.isOpened():\n",
    "        raise ValueError(f\"Could not open video: {video_path}\")\n",
    "    \n",
    "    frame_idx = 0\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "            \n",
    "        yield frame_idx, frame\n",
    "        frame_idx += 1\n",
    "    \n",
    "    cap.release()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34ac4519",
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_frame(frame, frame_idx, plot_img_oa, plot_img_wh, \n",
    "                  bodypart_x, bodypart_y, median_x, median_y, roi_x, roi_y, radius):\n",
    "    \n",
    "    annotated_video_frame = frame.copy()\n",
    "    \n",
    "   \n",
    "    if bodypart_x is not None and bodypart_y is not None and frame_idx < len(bodypart_x):\n",
    "       \n",
    "        for x_pos, y_pos in zip(bodypart_x[frame_idx], bodypart_y[frame_idx]):\n",
    "            if not (np.isnan(x_pos) or np.isnan(y_pos)): \n",
    "                cv2.circle(annotated_video_frame, \n",
    "                         (int(x_pos), int(y_pos)), 3, (255, 0, 0), -1)\n",
    "    \n",
    "        if not (np.isnan(median_x[frame_idx]) or np.isnan(median_y[frame_idx])): \n",
    "                cv2.circle(annotated_video_frame, \n",
    "                         (int(median_x[frame_idx]), int(median_y[frame_idx])), 3, (255, 0, 0), -1)\n",
    "    \n",
    "        \n",
    "        if not (np.isnan(roi_x) or np.isnan(roi_y)):\n",
    "            cv2.circle(annotated_video_frame, \n",
    "                     (int(roi_x), int(roi_y)), int(radius), (0, 255, 0), 2)\n",
    "            \n",
    "            cv2.circle(annotated_video_frame, \n",
    "                     (int(roi_x), int(roi_y)), 5, (0, 0, 255), -1)\n",
    "    \n",
    "    \n",
    "    plots_combined = np.vstack([plot_img_oa, plot_img_wh])\n",
    "    processed_frame = np.vstack([annotated_video_frame, plots_combined])\n",
    "    \n",
    "    return processed_frame\n",
    "\n",
    "\n",
    "def write_annotated_video(input_video_path, output_video_path, \n",
    "                         oa_speed, wh_speed, oa_onsets, oa_offsets, wh_onsets, wh_offsets, \n",
    "                         bodypart_x, bodypart_y, median_x, median_y,\n",
    "                         roi_x, roi_y, radius,  start_frame=0, end_frame=None, window_size=200):\n",
    "    \n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    cap.release()\n",
    "\n",
    "    \n",
    "    if end_frame is None:\n",
    "        end_frame = total_frames\n",
    "\n",
    "    (fig_oa, ax_oa, canvas_oa, line_oa, y_lim_oa,\n",
    "     fig_wh, ax_wh, canvas_wh, line_wh, y_lim_wh) = setup_plots(oa_speed, wh_speed)\n",
    "    \n",
    "    writer = None\n",
    "    \n",
    "    try:\n",
    "        for frame_idx, frame in read_video_frames(input_video_path):\n",
    "            if frame_idx < start_frame:\n",
    "                continue\n",
    "            if frame_idx >= end_frame:\n",
    "                break\n",
    "\n",
    "            plot_idx = frame_idx - start_frame\n",
    "            \n",
    "            window_start = max(0, frame_idx - window_size)\n",
    "            window_end = frame_idx\n",
    "            \n",
    "            \n",
    "            update_plot_window(ax_oa, ax_wh, line_oa, line_wh, y_lim_oa, y_lim_wh,\n",
    "                             window_start, window_end, oa_onsets, oa_offsets, \n",
    "                             wh_onsets, wh_offsets)\n",
    "            plot_img_oa, plot_img_wh = get_plot_images(canvas_oa, canvas_wh)\n",
    "            \n",
    "           \n",
    "            annotated_frame = annotate_frame(\n",
    "                frame, frame_idx, plot_img_oa, plot_img_wh,\n",
    "                bodypart_x, bodypart_y, median_x, median_y, \n",
    "                roi_x, roi_y, radius\n",
    "            )\n",
    "\n",
    "            if writer is None:\n",
    "                h, w = annotated_frame.shape[:2]\n",
    "                fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "                writer = cv2.VideoWriter(output_video_path, fourcc, fps, (w, h))\n",
    "            \n",
    "            writer.write(annotated_frame)\n",
    "            \n",
    "            if frame_idx % 100 == 0:\n",
    "                print(f\"Processed frame {frame_idx}/{end_frame}\")\n",
    "    \n",
    "    finally:\n",
    "        if writer:\n",
    "            writer.release()\n",
    "        plt.close(fig_oa)\n",
    "        plt.close(fig_wh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f7f8fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neck: 482 NaN frames for x (0.3%)\n",
      "neck: 482 NaN frames for y (0.3%)\n",
      "mid_back: 1371 NaN frames for x (0.9%)\n",
      "mid_back: 1371 NaN frames for y (0.9%)\n",
      "mouse_center: 206 NaN frames for x (0.1%)\n",
      "mouse_center: 206 NaN frames for y (0.1%)\n",
      "mid_backend: 460 NaN frames for x (0.3%)\n",
      "mid_backend: 460 NaN frames for y (0.3%)\n",
      "mid_backend2: 1770 NaN frames for x (1.2%)\n",
      "mid_backend2: 1770 NaN frames for y (1.2%)\n",
      "mid_backend3: 2292 NaN frames for x (1.5%)\n",
      "mid_backend3: 2292 NaN frames for y (1.5%)\n",
      "Processing frame 10/10\n",
      "151000\n",
      "150275\n",
      "151000\n",
      "No probe1 data found\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    \n",
    "\n",
    "sesh = load_and_process_session(subject_id, date, target_freq=60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0b80d35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bodypart_x type: <class 'pandas.core.frame.DataFrame'>\n",
      "bodypart_y type: <class 'pandas.core.frame.DataFrame'>\n",
      "Shape: (151000, 6)\n"
     ]
    }
   ],
   "source": [
    "print(f\"bodypart_x type: {type(sesh.bodypart_x)}\")\n",
    "print(f\"bodypart_y type: {type(sesh.bodypart_y)}\")\n",
    "print(f\"Shape: {sesh.bodypart_x.shape}\")\n",
    "body_x = sesh.bodypart_x.values  \n",
    "body_y = sesh.bodypart_y.values  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fb2c2c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed frame 800/5726\n",
      "Processed frame 900/5726\n",
      "Processed frame 1000/5726\n",
      "Processed frame 1100/5726\n",
      "Processed frame 1200/5726\n",
      "Processed frame 1300/5726\n",
      "Processed frame 1400/5726\n",
      "Processed frame 1500/5726\n",
      "Processed frame 1600/5726\n",
      "Processed frame 1700/5726\n",
      "Processed frame 1800/5726\n",
      "Processed frame 1900/5726\n",
      "Processed frame 2000/5726\n",
      "Processed frame 2100/5726\n",
      "Processed frame 2200/5726\n",
      "Processed frame 2300/5726\n",
      "Processed frame 2400/5726\n",
      "Processed frame 2500/5726\n",
      "Processed frame 2600/5726\n",
      "Processed frame 2700/5726\n",
      "Processed frame 2800/5726\n",
      "Processed frame 2900/5726\n",
      "Processed frame 3000/5726\n",
      "Processed frame 3100/5726\n",
      "Processed frame 3200/5726\n",
      "Processed frame 3300/5726\n",
      "Processed frame 3400/5726\n",
      "Processed frame 3500/5726\n",
      "Processed frame 3600/5726\n",
      "Processed frame 3700/5726\n",
      "Processed frame 3800/5726\n",
      "Processed frame 3900/5726\n",
      "Processed frame 4000/5726\n",
      "Processed frame 4100/5726\n",
      "Processed frame 4200/5726\n",
      "Processed frame 4300/5726\n",
      "Processed frame 4400/5726\n",
      "Processed frame 4500/5726\n",
      "Processed frame 4600/5726\n",
      "Processed frame 4700/5726\n",
      "Processed frame 4800/5726\n",
      "Processed frame 4900/5726\n",
      "Processed frame 5000/5726\n",
      "Processed frame 5100/5726\n",
      "Processed frame 5200/5726\n",
      "Processed frame 5300/5726\n",
      "Processed frame 5400/5726\n",
      "Processed frame 5500/5726\n",
      "Processed frame 5600/5726\n",
      "Processed frame 5700/5726\n"
     ]
    }
   ],
   "source": [
    "\n",
    "write_annotated_video(input_path, output_path, \n",
    "                         sesh.oa_speed, sesh.wh_speed, sesh.oa_onsets, sesh.oa_offsets, sesh.wh_onsets, sesh.wh_offsets,\n",
    "                         body_x, body_y, sesh.x, sesh.y,\n",
    "                         sesh.roi_x, sesh.roi_y, sesh.radius, start_frame=sesh.exp_onset, end_frame=sesh.exp_onset+5000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PinkRigs_data_analysis_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
